\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{dsfont}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{DS-GA 1003}
\newcommand\hwnumber{4}                  % <-- homework number
\newcommand\NetIDa{Cody Fizette}           % <-- NetID of person #1
\newcommand\NetIDb{netid12038}           % <-- NetID of person #2 (Comment this line out for problem sets)
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}
\global\long\def\minimizer#1{#1_{*}}
\global\long\def\ex{\mathbb{E}}
\global\long\def\nll{\text{NLL}}

\newenvironment{solution}[2][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}


\section*{2. From Scores to Conditional Probabilities}

\begin{problem}{2.1}
    Write $\ex_{y}\left[\ell\left(yf(x)\right)\mid x\right]$ in terms
    of $\pi(x)$, $\ell(-f(x))$, and $\ell\left(f(x)\right)$.
\end{problem}
\begin{solution}{}
    \begin{align*}
        \ex_{y}\left[\ell\left(yf(x)\right)\mid x\right] &= \pi(x)l(f(x)) + (1-\pi(x))l(-f(x))
    \end{align*}
\end{solution}

\begin{problem}{2.2}
    Show that the Bayes prediction function $f^{*}(x)$ for the exponential
    loss function $\ell\left(y,f(x)\right)=e^{-yf(x)}$ is given by 
    \[
    f^{*}(x)=\frac{1}{2}\ln\left(\frac{\pi(x)}{1-\pi(x)}\right),
    \]
    where we've assumed $\pi(x)\in\left(0,1\right)$. Also, show that
    given the Bayes prediction function $f^{*}$, we can recover the conditional
    probabilities by
    \[
    \pi(x)=\frac{1}{1+e^{-2f^{*}(x)}}.
    \]
\end{problem}
\begin{solution}{}
    \begin{align*}
        f^*(x) = argmin \pi(x)e^{-f(x)} + (1-\pi(x))e^{f(x)}
    \end{align*}
    Taking the derivative and setting this equal to 0 we get
    \begin{align*}
        -\pi(x)e^{-f^*(x)} + (1-\pi(x))e^{f^*(x)} &= 0\\
        (1-\pi(x))e^{f^*(x)} &= \pi(x)e^{-f^*(x)}\\
        e^{2f^*(x)} &= \frac{\pi(x)}{1-\pi(x)}\\
        2f^*(x) &= \ln\bigg(\frac{\pi(x)}{1-\pi(x)}\bigg)\\
        f^*(x) &= \frac{1}{2}\ln\bigg(\frac{\pi(x)}{1-\pi(x)}\bigg)
    \end{align*}
    For the second part observe that
    \begin{align*}
         e^{2f^*(x)} &= \frac{\pi(x)}{1-\pi(x)}\\
         e^{2f^*(x)} - \pi(x)e^{2f^*(x)} &= \pi(x)\\
         e^{2f^*(x)} &= \pi(x)(1+e^{2f^*(x)})\\
         \frac{1}{e^{-2f^*(x)}} &= \pi(x)(1+e^{2f^*(x)})\\
         \frac{1}{(1+e^{-2f^*(x)})} &= \pi(x)
    \end{align*}
\end{solution}
\newpage

\begin{problem}{2.3}
    Show that the Bayes prediction function $f^{*}(x)$ for the logistic
    loss function $\ell\left(y,f(x)\right)=\ln\left(1+e^{-yf(x)}\right)$
    is given by
    \[
    f^{*}(x)=\ln\left(\frac{\pi(x)}{1-\pi(x)}\right)
    \]
    and the conditional probabilities are given by
    \[
    \pi(x)=\frac{1}{1+e^{-f^{*}(x)}}.
    \]
\end{problem}
\begin{solution}{}
    \begin{align*}
        f^*(x) = argmin \pi(x)\ln(1+e^{-\hat{y}}) + (1-\pi(x))\ln(1+e^{\hat{y}})
    \end{align*}
    Taking the derivative and setting this equal to 0 we get
    \begin{align*}
        \frac{-\pi(x)e^{-\hat{y}}}{1+e^{-\hat{y}}} + \frac{(1-\pi(x))e^{\hat{y}}}{1+e^{\hat{y}}} &= 0\\
        \frac{-\pi(x)}{1+e^{\hat{y}}} + \frac{(1-\pi(x))e^{\hat{y}}}{1+e^{\hat{y}}} &= 0\\
        -\pi(x) + (1-\pi(x))e^\hat{y} &= 0\\
        e^\hat{y} &= \frac{\pi(x)}{1-\pi(x)}\\
        \hat{y} = f^*(x) &= \ln\bigg(\frac{\pi(x)}{1-\pi(x)}\bigg)
    \end{align*}
    For the second part observe that
    \begin{align*}
         e^\hat{y} &= \frac{\pi(x)}{1-\pi(x)}\\
         e^\hat{y} - \pi(x)e^\hat{y} &= \pi(x)\\
         e^\hat{y} &= \pi(x)(1+e^\hat{y})\\
         \frac{e^\hat{y}}{1+e^\hat{y}} &= \pi(x)\\
         \frac{1}{1+e^{-\hat{y}}} = \frac{1}{1+e^{-f^*(x)}} &= \pi(x)
    \end{align*}
\end{solution}
\newpage

\section*{3. Logistic Regression}
\begin{problem}{3.1}
    Show that $n\hat{R}_{n}(w)=\nll(w)$ for all $w\in\reals^{d}$.
    And thus the two approaches are equivalent, in that they produce the
    same prediction functions.
\end{problem}
\begin{solution}{}
    Recall that
    \begin{align*}
        n\hat{R}_{n}(w) & = \sum_{i=1}^{n}\log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right)\\
        \nll(w) & = \sum_{i=1}^{n}\left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right)
    \end{align*}
    Let $(x,y) \in D$ and $(x,y') \in D'$\\
    We will consider the cases where $y=1$ and $y=-1$ separately.\\
    
    \textbf{Case 1:} $y=-1 \implies y'=0$\\
    Observe that
    \begin{align*}
        \log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &= \log\left(1+\exp\left(w^{T}x_{i}\right)\right)
    \end{align*}
    And that
    \begin{align*}
        \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right) &= -\log(1-\phi(w^Tx))\\
        &= -\log\left(1-\frac{1}{1+e^{-w^Tx}}\right)\\
        &= -\log(\frac{e^{-w^Tx}}{1+e^{-w^Tx}})\\
        &= -\log(\frac{1}{1+e^{w^Tx}})\\
        &= \log(1+e^{w^Tx})
    \end{align*}
    Thus $\log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &=  \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right)$
    when $y=-1$\\
    
    \textbf{Case 2:} $y=1 \implies y'=1$\\
    Observe that
    \begin{align*}
        \log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &= \log\left(1+\exp\left(-w^{T}x_{i}\right)\right)
    \end{align*}
    And that
    \begin{align*}
        \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right) &= -\log(\phi(w^Tx))\\
        &= \log\left(\frac{1}{1+e^{-w^Tx}}\right)\\
        &= \log(1+e^{-w^Tx})
    \end{align*}
    Thus $\log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &=  \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right)$
    when $y=1$\\
    
    We have shown that all terms of the summations are equal. Thus we can conclude that $n\hat{R}_{n}(w)=\nll(w)$.
\end{solution}
\newpage

\begin{problem}{3.2.1}
    Show that the expression for LogSumExp is valid.
\end{problem}
\begin{solution}{}
    \begin{align*}
        \log(e^{x_i} + \dots e^{x_n}) &= \log(e^{x^*}(e^{x_1-x^*} + \dots e^{x_n-x^*}))\\
        &= \log(e^{x^*}) + \log(e^{x_1-x^*} + \dots e^{x_n-x^*})\\
        &= x^* + \log(e^{x_1-x^*} + \dots e^{x_n-x^*})
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.2.2}
    Show that $\exp\left(x_{i}-x^{*}\right)\in(0,1]$ for any $i$, and thus the exp calculations will not overflow.
\end{problem}
\begin{solution}{}
    Since $x^*=max(x_1, \dots x_n)$, we have $x_i-x^*\leq 0$ for all $i$. Thus $0<e^{x_i-x^*}\leq1$ for all $i$
\end{solution}
\newpage

\begin{problem}{3.2.3}
    Explain why the $\log$ term in our expression
    $\log\left[e^{x_{1}-x^{*}}+\cdots+e^{x_{n}-x^{*}}\right]$ will never
    be ``-inf''.
\end{problem}
\begin{solution}{}
    Note that there exists at least one $x_i$ such that $x_i=x^*$ and that for such numbers $e^{x_i-x^*}=e^0=1$. 
    Thus $e^{x_1-x^*} + \dots e^{x_n-x^*} > 1$. Thus $\ln(e^{x_1-x^*} + \dots e^{x_n-x^*}) > e$ and so we don't have to worry
    about negative infinity.
\end{solution}
\newpage

\begin{problem}{3.2.4}
    Show how to use the numpy function\textit{ \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.logaddexp.html}{logaddexp}
    }\textit{\emph{to correctly compute $\log\left(1+e^{-s}\right)$}}
\end{problem}
\begin{solution}{}
    #TODO Triple check this\\
    np.logaddexp(0,-s)
\end{solution}
\end{document}
