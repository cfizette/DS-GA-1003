\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{dsfont}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{DS-GA 1003}
\newcommand\hwnumber{5}                  % <-- homework number
\newcommand\NetIDa{Cody Fizette}           % <-- NetID of person #1
\newcommand\NetIDb{netid12038}           % <-- NetID of person #2 (Comment this line out for problem sets)
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}
\global\long\def\minimizer#1{#1_{*}}
\global\long\def\ex{\mathbb{E}}
\global\long\def\nll{\text{NLL}}

\newenvironment{solution}[2][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}


\section*{2. From Scores to Conditional Probabilities}

\begin{problem}{2.1}
    Write $\ex_{y}\left[\ell\left(yf(x)\right)\mid x\right]$ in terms
    of $\pi(x)$, $\ell(-f(x))$, and $\ell\left(f(x)\right)$.
\end{problem}
\begin{solution}{}
    \begin{align*}
        \ex_{y}\left[\ell\left(yf(x)\right)\mid x\right] &= \pi(x)l(f(x)) + (1-\pi(x))l(-f(x))
    \end{align*}
\end{solution}

\begin{problem}{2.2}
    Show that the Bayes prediction function $f^{*}(x)$ for the exponential
    loss function $\ell\left(y,f(x)\right)=e^{-yf(x)}$ is given by 
    \[
    f^{*}(x)=\frac{1}{2}\ln\left(\frac{\pi(x)}{1-\pi(x)}\right),
    \]
    where we've assumed $\pi(x)\in\left(0,1\right)$. Also, show that
    given the Bayes prediction function $f^{*}$, we can recover the conditional
    probabilities by
    \[
    \pi(x)=\frac{1}{1+e^{-2f^{*}(x)}}.
    \]
\end{problem}
\begin{solution}{}
    \begin{align*}
        f^*(x) = argmin \pi(x)e^{-f(x)} + (1-\pi(x))e^{f(x)}
    \end{align*}
    Taking the derivative and setting this equal to 0 we get
    \begin{align*}
        -\pi(x)e^{-f^*(x)} + (1-\pi(x))e^{f^*(x)} &= 0\\
        (1-\pi(x))e^{f^*(x)} &= \pi(x)e^{-f^*(x)}\\
        e^{2f^*(x)} &= \frac{\pi(x)}{1-\pi(x)}\\
        2f^*(x) &= \ln\bigg(\frac{\pi(x)}{1-\pi(x)}\bigg)\\
        f^*(x) &= \frac{1}{2}\ln\bigg(\frac{\pi(x)}{1-\pi(x)}\bigg)
    \end{align*}
    For the second part observe that
    \begin{align*}
         e^{2f^*(x)} &= \frac{\pi(x)}{1-\pi(x)}\\
         e^{2f^*(x)} - \pi(x)e^{2f^*(x)} &= \pi(x)\\
         e^{2f^*(x)} &= \pi(x)(1+e^{2f^*(x)})\\
         \frac{1}{e^{-2f^*(x)}} &= \pi(x)(1+e^{2f^*(x)})\\
         \frac{1}{(1+e^{-2f^*(x)})} &= \pi(x)
    \end{align*}
\end{solution}
\newpage

\begin{problem}{2.3}
    Show that the Bayes prediction function $f^{*}(x)$ for the logistic
    loss function $\ell\left(y,f(x)\right)=\ln\left(1+e^{-yf(x)}\right)$
    is given by
    \[
    f^{*}(x)=\ln\left(\frac{\pi(x)}{1-\pi(x)}\right)
    \]
    and the conditional probabilities are given by
    \[
    \pi(x)=\frac{1}{1+e^{-f^{*}(x)}}.
    \]
\end{problem}
\begin{solution}{}
    \begin{align*}
        f^*(x) = argmin \pi(x)\ln(1+e^{-\hat{y}}) + (1-\pi(x))\ln(1+e^{\hat{y}})
    \end{align*}
    Taking the derivative and setting this equal to 0 we get
    \begin{align*}
        \frac{-\pi(x)e^{-\hat{y}}}{1+e^{-\hat{y}}} + \frac{(1-\pi(x))e^{\hat{y}}}{1+e^{\hat{y}}} &= 0\\
        \frac{-\pi(x)}{1+e^{\hat{y}}} + \frac{(1-\pi(x))e^{\hat{y}}}{1+e^{\hat{y}}} &= 0\\
        -\pi(x) + (1-\pi(x))e^\hat{y} &= 0\\
        e^\hat{y} &= \frac{\pi(x)}{1-\pi(x)}\\
        \hat{y} = f^*(x) &= \ln\bigg(\frac{\pi(x)}{1-\pi(x)}\bigg)
    \end{align*}
    For the second part observe that
    \begin{align*}
         e^\hat{y} &= \frac{\pi(x)}{1-\pi(x)}\\
         e^\hat{y} - \pi(x)e^\hat{y} &= \pi(x)\\
         e^\hat{y} &= \pi(x)(1+e^\hat{y})\\
         \frac{e^\hat{y}}{1+e^\hat{y}} &= \pi(x)\\
         \frac{1}{1+e^{-\hat{y}}} = \frac{1}{1+e^{-f^*(x)}} &= \pi(x)
    \end{align*}
\end{solution}
\newpage

\section*{3. Logistic Regression}
\begin{problem}{3.1}
    Show that $n\hat{R}_{n}(w)=\nll(w)$ for all $w\in\reals^{d}$.
    And thus the two approaches are equivalent, in that they produce the
    same prediction functions.
\end{problem}
\begin{solution}{}
    Recall that
    \begin{align*}
        n\hat{R}_{n}(w) & = \sum_{i=1}^{n}\log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right)\\
        \nll(w) & = \sum_{i=1}^{n}\left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right)
    \end{align*}
    Let $(x,y) \in D$ and $(x,y') \in D'$\\
    We will consider the cases where $y=1$ and $y=-1$ separately.\\
    
    \textbf{Case 1:} $y=-1 \implies y'=0$\\
    Observe that
    \begin{align*}
        \log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &= \log\left(1+\exp\left(w^{T}x_{i}\right)\right)
    \end{align*}
    And that
    \begin{align*}
        \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right) &= -\log(1-\phi(w^Tx))\\
        &= -\log\left(1-\frac{1}{1+e^{-w^Tx}}\right)\\
        &= -\log(\frac{e^{-w^Tx}}{1+e^{-w^Tx}})\\
        &= -\log(\frac{1}{1+e^{w^Tx}})\\
        &= \log(1+e^{w^Tx})
    \end{align*}
    Thus $\log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &=  \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right)$
    when $y=-1$\\
    
    \textbf{Case 2:} $y=1 \implies y'=1$\\
    Observe that
    \begin{align*}
        \log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &= \log\left(1+\exp\left(-w^{T}x_{i}\right)\right)
    \end{align*}
    And that
    \begin{align*}
        \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right) &= -\log(\phi(w^Tx))\\
        &= \log\left(\frac{1}{1+e^{-w^Tx}}\right)\\
        &= \log(1+e^{-w^Tx})
    \end{align*}
    Thus $\log\left(1+\exp\left(-y_{i}w^{T}x_{i}\right)\right) &=  \left[-y_{i}'\log\phi(w^{T}x_{i})\right]+\left(y_{i}'-1\right)\log\left(1-\phi(w^{T}x_{i})\right)$
    when $y=1$\\
    
    We have shown that all terms of the summations are equal. Thus we can conclude that $n\hat{R}_{n}(w)=\nll(w)$.
\end{solution}
\newpage

\begin{problem}{3.2.1}
    Show that the expression for LogSumExp is valid.
\end{problem}
\begin{solution}{}
    \begin{align*}
        \log(e^{x_i} + \dots e^{x_n}) &= \log(e^{x^*}(e^{x_1-x^*} + \dots e^{x_n-x^*}))\\
        &= \log(e^{x^*}) + \log(e^{x_1-x^*} + \dots e^{x_n-x^*})\\
        &= x^* + \log(e^{x_1-x^*} + \dots e^{x_n-x^*})
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.2.2}
    Show that $\exp\left(x_{i}-x^{*}\right)\in(0,1]$ for any $i$, and thus the exp calculations will not overflow.
\end{problem}
\begin{solution}{}
    Since $x^*=max(x_1, \dots x_n)$, we have $x_i-x^*\leq 0$ for all $i$. Thus $0<e^{x_i-x^*}\leq1$ for all $i$
\end{solution}
\newpage

\begin{problem}{3.2.3}
    Explain why the $\log$ term in our expression
    $\log\left[e^{x_{1}-x^{*}}+\cdots+e^{x_{n}-x^{*}}\right]$ will never
    be ``-inf''.
\end{problem}
\begin{solution}{}
    Note that there exists at least one $x_i$ such that $x_i=x^*$ and that for such numbers $e^{x_i-x^*}=e^0=1$. 
    Thus $e^{x_1-x^*} + \dots e^{x_n-x^*} > 1$. Thus $\ln(e^{x_1-x^*} + \dots e^{x_n-x^*}) > e$ and so we don't have to worry
    about negative infinity.
\end{solution}
\newpage

\begin{problem}{3.2.4}
    Show how to use the numpy function\textit{ \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.logaddexp.html}{logaddexp}
    }\textit{\emph{to correctly compute $\log\left(1+e^{-s}\right)$}}
\end{problem}
\begin{solution}{}
    np.logaddexp(0,-s)
\end{solution}
\newpage

\section*{4. Bayesian Logistic Regression with Gaussian Priors}
\begin{problem}{4.1}
    For the dataset $\cd'$ described in Section 3.1,
    give an expression for the posterior density $p(w\mid\cd')$ in terms
    of the negative log-likelihood function
\end{problem}
\begin{solution}{}
    \begin{align*}
        p(w\mid D') \propto p(w)e^{-NLL(w)}
    \end{align*}
\end{solution}
\newpage

\begin{problem}{4.2}
    Suppose we take a prior on $w$ of the form $w\sim\cn(0,\Sigma)$.
    Find a covariance matrix $\Sigma$ such that MAP estimate for $w$
    after observing data $\cd'$ is the same as the minimizer of the regularized
    logistic regression function defined in Section 3.3
    (and prove it).
\end{problem}
\begin{solution}{}
    From section 3.3 we have
    \begin{align*}
        \hat{w} = \text{argmin}\;\hat{R_n}(w)+\lambda\|w\|^{2}.
    \end{align*}
    Now
    \begin{align*}
        w^* &= \text{argmax}\; p(w\mid D')\\
        &= \text{argmax}\; p(w)e^{-NLL(w)}\\
        &= \text{argmin}\;-\ln\left(p(w)e^{-NLL(w)}\right)\\
        &= \text{argmin}\; \frac{1}{2}w^T\Sigma^{-1}w + NLL(w)\\
        &= \text{argmin}\; \frac{1}{2n}w^T\Sigma^{-1}w + \hat{R_n}(w) && \text{since $NLL(w)=n\hat{R_n}(w)$}
    \end{align*}
    Thus
    \begin{align*}
        \lambda w^Tw &= \frac{1}{2n}w^T\Sigma^{-1}w\\
        \lambda I &= \frac{1}{2n}w^T\Sigma^{-1}\\
        \Sigma &= \frac{1}{2n\lambda}I
    \end{align*}
\end{solution}
\newpage

\begin{problem}{4.3}
    In the Bayesian approach, the prior should reflect your beliefs about
    the parameters before seeing the data and, in particular, should be
    independent on the eventual size of your dataset. Following this,
    you choose a prior distribution $w\sim\cn(0,I)$. For a dataset $\cd$
    of size $n$, how should you choose $\lambda$ in our regularized
    logistic regression objective function so that the minimizer is equal
    to the mode of the posterior distribution of $w$ (i.e. is equal to
    the MAP estimator).
\end{problem}
\begin{solution}{}
    \begin{align*}
        \lambda = \frac{1}{2n}
    \end{align*}
\end{solution}
\newpage


\section*{6. Coin Flipping Maximum Likelihood}
\begin{problem}{6.1}
    Suppose we flip a coin and get the following sequence
    of heads and tails:
    \[
    \cd=(H,H,T)
    \]
    Give an expression for the probability of observing $\cd$ given that
    the probability of heads is $\theta$. That is, give an expression
    for $p\left(\cd\mid\theta\right)$. This is called the \textbf{likelihood
    of $\theta$ for the data $\cd$}.
\end{problem}
\begin{solution}{}
    \begin{align*}
        p(\cd\mid\theta) = \theta^2(1-\theta)
    \end{align*}
\end{solution}
\newpage

\begin{problem}{6.2}
    How many different sequences of 3 coin tosses have
    $2$ heads and 1 tail? If we toss the coin $3$ times, what is the
    probability of 2 heads and $1$ tail? (Answer should be in terms of
    $\theta$.)
\end{problem}
\begin{solution}{}
    \begin{align*}
        p(2H,1T) = 3\theta^2(1-\theta)
    \end{align*}
\end{solution}
\newpage

\begin{problem}{6.3}
    More generally, give an expression for the likelihood
    $p(\cd\mid\theta)$ for a particular sequence of flips $\cd$ that
    has $n_{h}$ heads and $n_{t}$ tails. Make sure you have expressions
    that make sense even for $\theta=0$ and $n_{h}=0$, and other boundary
    cases. You may use the convention that $0^{0}=1$, or you can break
    your expression into cases if needed.
\end{problem}
\begin{solution}{}
    \begin{align*}
        p(\cd\mid\theta) = \theta^{n_h}(1-\theta)^{n_t}
    \end{align*}
\end{solution}
\newpage

\begin{problem}{6.4}
    Show that the maximum likelihood estimate of $\theta$
    given we observed a sequence with $n_{h}$ heads and $n_{t}$ tails
    is
    \[
    \hat{\theta}_{\text{MLE}}=\frac{n_{h}}{n_{h}+n_{t}}.
    \]
    You may assume that $n_{h}+n_{t}\ge1$. (Hint: Maximizing the log-likelihood
    is equivalent and is often easier. )
\end{problem}
\begin{solution}{}
    \begin{align*}
        \hat{\theta}_{MLE} = argmax\; \theta^{n_h}(1-\theta)^{n_t} = argmax\; n_h\ln(\theta) + n_t\ln(1-\theta)
    \end{align*}
    Taking the derivative and setting it to 0 we get
    \begin{align*}
        \frac{n_h}{\hat{\theta}} - \frac{n_t}{1-\hat{\theta}} &= 0\\
        n_h(1-\hat{\theta}) - n_t\hat{\theta} &= 0\\
        n_h - n_h\hat{\theta} - n_t\hat{\theta} &= 0\\
        \hat{\theta} &= \frac{n_h}{n_h+n_t}
    \end{align*}
\end{solution}
\newpage

\section*{7. Coin Flipping: Bayesian Approach with Beta Prior}
\begin{problem}{7.1}
    Suppose that our prior distribution on $\theta$ is
    $\mbox{Beta}(h,t)$, for some $h,t>0$. That is, $p(\theta)\propto\theta^{h-1}\left(1-\theta\right)^{t-1}$.
    Suppose that our sequence of flips $\cd$ has $n_{h}$ heads and $n_{t}$
    tails. Show that the posterior distribution for $\theta$ is $\mbox{Beta}(h+n_{h},t+n_{t})$.
    That is, show that
    \[
    p(\theta\mid\cd)\propto\theta^{h-1+n_{h}}\left(1-\theta\right)^{t-1+n_{t}}.
    \]
\end{problem}
\begin{solution}{}
    \begin{align*}
        p(\theta) &= \theta^{h-1}(1-\theta)^{t-1}\\
        \\
        p(\theta\mid\cd) &\propto \p(\theta)\p(\cd\mid\theta)\\
        &= \theta^{h-1}(1-\theta)^{t-1}\theta^{n_h}(1-\theta)^{n_t}\\
        &= \theta^{h+n_h-1}(1-\theta)^{t+n_t-1}
    \end{align*}
\end{solution}
\newpage

\begin{problem}{7.2}
    Give expressions for the MLE, the MAP, and the posterior mean estimates of $\theta$.
\end{problem}
\begin{solution}{}
    \begin{align*}
        MLE &= \frac{n_h}{n_h+n_t}\\
        MAP &= \frac{h+n_h-1}{h+t+n_h+n_t-2}\\
        Posterior Mean &= \frac{h_n_h}{h+t+n_h+n_t} 
    \end{align*}
\end{solution}
\newpage

\begin{problem}{7.3}
    What happens to $\hat{\theta}_{\text{MLE }}$, $\hat{\theta}_{\text{MAP}}$,
    and $\hat{\theta}_{\text{POSTERIOR MEAN}}$ as the number of coin
    flips $n=n_{h}+n_{t}$ approaches infinity?
\end{problem}
\begin{solution}{}
    They all approach the same value. $MLE = MAP = Posterior Mean = \frac{n_h}{n_h+n_t}$
\end{solution}
\newpage

\begin{problem}{7.4}
    The MAP and posterior mean estimators of $\theta$
    were derived from a Bayesian perspective. Let's now evaluate them
    from a frequentist perspective. Suppose $\theta$ is fixed and unknown.
    Which of the MLE, MAP, and posterior mean estimators give \textbf{unbiased}
    estimates of $\theta$, if any?
\end{problem}
\begin{solution}{}
    The MLE  is always unbiased and the MAP is unbiased only if $h=t=1$.
    \begin{align*}
        E(MLE) &= \frac{1}{n}E(n_h) = \frac{1}{n}n\theta = \theta\\
        \\
        E(MAP) &= \frac{h-1}{n+h+t-2} + \frac{n\theta}{n+h+t-2} = \theta &&\text{when $h=t=1$}\\
        \\
        E(Posterior Mean) &= \frac{h}{n+h+t} + \frac{n\theta}{n+h+t} \neq \theta &&\text{since $h,t>0$}
    \end{align*}
\end{solution}
\newpage

\begin{problem}{7.5}
     Suppose somebody gives you a coin and asks you to give
    an estimate of the probability of heads, but you can only toss the
    coin $3$ times. You have no particular reason to believe this is
    an unfair coin. Would you prefer the MLE or the posterior mean as
    a point estimate of $\theta$? If the posterior mean, what would you
    use for your prior?
\end{problem}
\begin{solution}{}
    I would prefer the posterior mean. My belief is that most coins are close to fair. With a sample size of only 3, the closest to 
    $\theta=0.5$ we could get is $\theta=\frac{1}{3}$ or $\theta=\frac{2}{3}$. I think that it is very unlikely that even a trick coin could 
    be that far from fair. For a prior, I would choose a Beta distribution centered at 0.5 with low variance, something like $Beta(100,100)$.
\end{solution}
\newpage

\section*{8. Hierarchical Bayes for Click-Trhough Rate Estimation}
\begin{problem}{8.1.1}
     Give an expression for $p(\cd_{i}\mid\theta_{i})$, the likelihood
    of $\cd_{i}$ given the probability of click $\theta_{i}$, in terms
    of $\theta_{i}$, $x_{i}$ and $n_{i}$.
\end{problem}
\begin{solution}{}
    \begin{align*}
        p(D_i\mid\theta_i) = \theta^{x_i}(1-\theta)^{n_I-x_i}
    \end{align*}
\end{solution}
\newpage

\begin{problem}{8.1.2}
    We will take our prior distribution on $\theta_{i}$ to be $\mbox{Beta}(a,b)$.
    The corresponding probability density function is given by
    \[
    p(\theta_{i})=\mbox{Beta}(\theta_{i};a,b)=\frac{1}{B(a,b)}\theta_{i}^{a-1}\left(1-\theta_{i}\right)^{b-1},
    \]
    where $B(a,b)$ is called the Beta function. Explain (without calculation)
    why we must have
    \[
    \int\theta_{i}^{a-1}\left(1-\theta_{i}\right)^{b-1}\,d\theta_{i}=B(a,b).
    \]
\end{problem}
\begin{solution}{}
    The area under the PDF must equal 1.
\end{solution}
\newpage

\begin{problem}{8.1.3}
    Give an expression for the posterior distribution $p(\theta_{i}\mid\cd_{i})$.
    In this case, include the constant of proportionality. In other words,
    do not use the ``is proportional to'' sign $\propto$ in your final
    expression.
\end{problem}
\begin{solution}{}
    Note that
    \begin{align*}
        p(\theta_i\mid D_i) \propto \theta_i^{a+x_i-1}(1-\theta_i)^{n_i+b-x_i-1}
    \end{align*}
    Now
    \begin{align*}
        \int \theta_i^{a+x_i-1}(1-\theta_i)^{n_i+b-x_i-1} d\theta_i = B(a+x_i, n_i+b-x_i)
    \end{align*}
    Thus, in order to have a valid PDF,
    \begin{align*}
        p(\theta_i\mid D_i) = \frac{1}{B(a+x_i, n_i+b-x_i)}\theta_i^{a+x_i-1}(1-\theta_i)^{n_i+b-x_i-1}
    \end{align*}
\end{solution}
\newpage

\begin{problem}{8.1.4}
    Give a closed form expression for $p(\cd_{i})$, the marginal likelihood
    of $\cd_{i}$, in terms of the $a,b,x_{i},$ and $n_{i}$. You may
    use the normalization function $B(\cdot,\cdot)$ for convenience,
    but you should not have any integrals in your solution.
\end{problem}
\begin{solution}{}
    \begin{align*}
        p(D_i) &= \int p(D_i\mid\theta_i)p(\theta_i)d\theta_i\\
        &= \int \frac{1}{B(a,b)}\theta_i^{a+x_i-1}(1-\theta_i)^{n_i+b-x_i-1}d\theta_i\\
        &= \frac{B(a+x_i, n_i+b-x_i)}{B(a,b)}
    \end{align*}
\end{solution}
\newpage

\begin{problem}{8.1.4}
    The maximum likelihood estimate for $\theta_{i}$ is $x_{i}/n_{i}$.
    Let $p_{\text{MLE}}(\cd_{i})$ be the marginal likelihood of $\cd_{i}$
    when we use a prior on $\theta_{i}$ that puts all of its probability
    mass at $x_{i}/n_{i}$. Note that 
    \begin{eqnarray*}
    p_{\text{MLE}}(\cd_{i}) & = & p\left(\cd_{i}\mid\theta_{i}=\frac{x_{i}}{n_{i}}\right)p\left(\theta_{i}=\frac{x_{i}}{n_{i}}\right)\\
     & = & p\left(\cd_{i}\mid\theta_{i}=\frac{x_{i}}{n_{i}}\right).
    \end{eqnarray*}
    Explain why, or prove, that $p_{\text{MLE}}(\cd_{i})$ is larger than
    $p(\cd_{i})$ for any other prior we might put on $\theta_{i}$.
\end{problem}
\begin{solution}{}
    By definition, $\theta_{MLE} = \frac{x_i}{n} = \text{argmax}\; p(D_i\mid\theta)$.\\
    Thus $p(D_i\mid\theta_{MLE}) \geq p(D_i\mid\theta) \;\; \forall \;\; \theta$\\
    Therefore $p(\cd_{i})=\int p\left(\cd_{i}\mid\theta_{i}\right)p(\theta_{i})\,d\theta_{i}$ is maximized when 
     \[ p(\theta) =  \begin{cases} 
      1 & \theta = \frac{x_i}{n} \\
      0 & \text{otherwise}
   \end{cases}
    \]
\end{solution}
\newpage

\begin{problem}{8.1.5}
    Explain what's happening to the prior as we continue to increase the likelihood.
\end{problem}
\begin{solution}{}
    The prior's mode approached the MLE of $\theta$ and the variance approaches 0.
\end{solution}
\newpage

\end{document}
