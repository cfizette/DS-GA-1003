{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormPenaltyNode(object):\n",
    "    \"\"\" Node computing l2_reg * ||w||^2 for scalars l2_reg and vector w\"\"\"\n",
    "    def __init__(self, l2_reg, w, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        l2_reg: a scalar value >=0 (not a node)\n",
    "        w: a node for which w.out is a numpy vector\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.l2_reg = np.array(l2_reg)\n",
    "        self.w = w\n",
    "        \n",
    "    def forward(self):\n",
    "        self.out = self.l2_reg * (self.w.out @ self.w.out)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        d_w = self.d_out * 2 * self.l2_reg * self.w.out\n",
    "        self.w.d_out += d_w\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumNode(object):\n",
    "    \"\"\" Node computing a + b, for numpy arrays a and b\"\"\"\n",
    "    def __init__(self, a, b, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "        b: node for which b.out is a numpy array of the same shape as a\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = self.a.out + self.b.out\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        d_a = self.d_out\n",
    "        d_b = self.d_out\n",
    "        self.a.d_out += d_a\n",
    "        self.b.d_out += d_b\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.a, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" Ridge regression with computation graph \"\"\"\n",
    "    def __init__(self, l2_reg=1, step_size=.005,  max_num_epochs = 5000):\n",
    "        self.max_num_epochs = max_num_epochs\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Build computation graph\n",
    "        self.x = nodes.ValueNode(node_name=\"x\") # to hold a vector input\n",
    "        self.y = nodes.ValueNode(node_name=\"y\") # to hold a scalar response\n",
    "        self.w = nodes.ValueNode(node_name=\"w\") # to hold the parameter vector\n",
    "        self.b = nodes.ValueNode(node_name=\"b\") # to hold the bias parameter (scalar)\n",
    "        self.prediction = nodes.VectorScalarAffineNode(x=self.x, w=self.w, b=self.b,\n",
    "                                                 node_name=\"prediction\")\n",
    "        self.square_loss = nodes.SquaredL2DistanceNode(a=self.prediction, b=self.y,\n",
    "                                                 node_name=\"square loss\")\n",
    "        self.reg = nodes.L2NormPenaltyNode(l2_reg=l2_reg, w=self.w, node_name='l2 regularization')\n",
    "        self.objective = nodes.SumNode(a = self.square_loss, b=self.reg, node_name = 'objective function')\n",
    "\n",
    "        # Group nodes into types to construct computation graph function\n",
    "        self.inputs = [self.x]\n",
    "        self.outcomes = [self.y]\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "        self.graph = graph.ComputationGraphFunction(self.inputs, self.outcomes,\n",
    "                                                          self.parameters, self.prediction,\n",
    "                                                          self.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3.1-ridge.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No regularization avg training loss:  0.031891987351410654"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization avg training loss:  0.20162615691882524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineNode(object):\n",
    "    \"\"\"Node implementing affine transformation (W,x,b)-->Wx+b, where W is a matrix,\n",
    "    and x and b are vectors\n",
    "        Parameters:\n",
    "        W: node for which W.out is a numpy array of shape (m,d)\n",
    "        x: node for which x.out is a numpy array of shape (d)\n",
    "        b: node for which b.out is a numpy array of shape (m) (i.e. vector of length m)\n",
    "    \"\"\"\n",
    "    def __init__(self, W, x, b, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "        b: node for which b.out is a numpy array of the same shape as a\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        self.x = x\n",
    "        self.b = b\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = self.W.out @ self.x.out + self.b.out\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        d_W = np.outer(self.d_out, self.x.out)\n",
    "\n",
    "        # Handle cases where W is an array vs a matrix\n",
    "        if len(self.W.out.shape) == 1:\n",
    "            d_x = self.W.out.T * self.d_out\n",
    "        else:\n",
    "            d_x = self.W.out.T @ self.d_out\n",
    "        d_b = self.d_out\n",
    "\n",
    "        # Reshape d_W when it is supposed to be an array\n",
    "        if d_W.shape[0] == 1:\n",
    "            d_W = d_W.flatten()\n",
    "        self.W.d_out += d_W\n",
    "        self.x.d_out += d_x\n",
    "        self.b.d_out += d_b\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.W, self.x, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhNode(object):\n",
    "    \"\"\"Node tanh(a), where tanh is applied elementwise to the array a\n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, a, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = np.tanh(self.a.out)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        d_a = self.d_out * (1 - self.out**2)\n",
    "        self.a.d_out += d_a\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" MLP regression with computation graph \"\"\"\n",
    "    def __init__(self, num_hidden_units=10, step_size=.005, init_param_scale=0.01, max_num_epochs = 5000):\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.init_param_scale = 0.01\n",
    "        self.max_num_epochs = max_num_epochs\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Build computation graph\n",
    "        self.x = nodes.ValueNode(node_name=\"x\") # to hold a vector input\n",
    "        self.y = nodes.ValueNode(node_name=\"y\") # to hold a scalar response\n",
    "        self.b1 = nodes.ValueNode(node_name='b1')\n",
    "        self.b2 = nodes.ValueNode(node_name='b2')\n",
    "        self.W1 = nodes.ValueNode(node_name='W1')\n",
    "        self.w2 = nodes.ValueNode(node_name='w2')\n",
    "        self.L = nodes.AffineNode(W=self.W1, x=self.x, b=self.b1, node_name='L')\n",
    "        self.h = nodes.TanhNode(a=self.L, node_name='L')\n",
    "        self.prediction = nodes.AffineNode(W=self.w2, x=self.h, b=self.b2, node_name='prediction')\n",
    "        self.objective = nodes.SquaredL2DistanceNode(self.y, self.prediction, node_name='objective')\n",
    "\n",
    "        self.inputs = [self.x]\n",
    "        self.outcomes = [self.y]\n",
    "        self.parameters = [self.W1, self.b1, self.w2, self.b2]\n",
    "\n",
    "        self.graph = graph.ComputationGraphFunction(self.inputs, self.outcomes,\n",
    "                                                          self.parameters, self.prediction,\n",
    "                                                          self.objective)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_instances, num_ftrs = X.shape\n",
    "        y = y.reshape(-1)\n",
    "\n",
    "        ## TODO: Initialize parameters (small random numbers -- not all 0, to break symmetry )\n",
    "        s = self.init_param_scale\n",
    "        init_values = {\"W1\": s*np.random.randn(self.num_hidden_units,num_ftrs), \n",
    "                       \"b1\": s*np.random.randn(self.num_hidden_units),\n",
    "                       \"b2\": s*np.random.randn(1), \n",
    "                       \"w2\": s*np.random.randn(self.num_hidden_units)\n",
    "                        }\n",
    "\n",
    "        self.graph.set_parameters(init_values)\n",
    "\n",
    "        for epoch in range(self.max_num_epochs):\n",
    "            shuffle = np.random.permutation(num_instances)\n",
    "            epoch_obj_tot = 0.0\n",
    "            #pdb.set_trace()\n",
    "            for j in shuffle:\n",
    "                obj, grads = self.graph.get_gradients(input_values = {\"x\": X[j]},\n",
    "                                                    outcome_values = {\"y\": np.array([y[j]])})\n",
    "                #print(obj)\n",
    "                epoch_obj_tot += obj\n",
    "                # Take step in negative gradient direction\n",
    "                steps = {}\n",
    "                for param_name in grads:\n",
    "                    steps[param_name] = -self.step_size * grads[param_name]\n",
    "                    self.graph.increment_parameters(steps)\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                train_loss = sum((y - self.predict(X,y)) **2)/num_instances\n",
    "                print(\"Epoch \",epoch,\": Ave objective=\",epoch_obj_tot/num_instances,\" Ave training loss: \",train_loss)\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"graph\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "\n",
    "        num_instances = X.shape[0]\n",
    "        preds = np.zeros(num_instances)\n",
    "        for j in range(num_instances):\n",
    "            preds[j] = self.graph.get_prediction(input_values={\"x\":X[j]})\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4.2.3-mlp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No features avg training loss:  0.21752394252822882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featurized avg training loss:  0.10853926132523047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
