\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{dsfont}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{DS-GA 1003}
\newcommand\hwnumber{4}                  % <-- homework number
\newcommand\NetIDa{Cody Fizette}           % <-- NetID of person #1
\newcommand\NetIDb{netid12038}           % <-- NetID of person #2 (Comment this line out for problem sets)

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}
\global\long\def\minimizer#1{#1_{*}}
\global\long\def\ex{\mathbb{E}}

\newenvironment{solution}[2][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}


\section*{2. Kernel Matrices}

\begin{problem}{2.1}

Consider a set of vectors $S=\{x_{1},\ldots,x_{m}\}$.
Let $X$ denote the matrix whose rows are these vectors. Form the
Gram matrix $K=XX^{T}$. Show that knowing $K$ is equivalent to knowing
the set of pairwise distances among the vectors in $S$ as well as
the vector lengths.
\end{problem}



\begin{solution}{}
From the definition of the Gram matrix, knowing $K$ is equivalent to knowing
$\langle x, x' \rangle\,\, \forall\,\, x,x' \in S$. Now let $x,x' \in S$ and observe that
\begin{align*}
    \langle x,x' \rangle &= \langle x'+(x-x'), x+(x'-x) \rangle\\
    &= \langle x', x\rangle + \langle x', x'\rangle + \langle x', -x\rangle + \langle x, x\rangle + \langle -x', x\rangle + \langle x - x', x'-x\rangle\\
    &= \|x\|^2 + \|x'\|^2 - \|x-x', x-x'\|^2 - \langle x',x \rangle\\
    &= \frac{\|x\|^2 + \|x'\|^2 - \|x-x', x-x'\|^2}{2}
\end{align*}

Thus it is possible to express elements of $K$ in terms of the distance between vectors in $S$ as well as the vector lengths. We can conclude that
knowing $K$ is equivalent to knowing the set of pairwise distances among the vectors in $S$ as well as the vector lengths.
\end{solution}
\newpage


\section*{3. Kernel Ridge Regression}
\begin{problem}{3.1}
Show that for $w$ to be a minimizer of $J(w)$, we must have $X^{T}Xw+\lambda Iw=X^{T}y$.
Show that the minimizer of $J(w)$ is $w=(X^{T}X+\lambda I)^{-1}X^{T}y$.
Justify that the matrix $X^{T}X+\lambda I$ is invertible, for $\lambda>0$.
\end{problem}

\begin{solution}{}
$\,$
\begin{enumerate}
    \item For $w$ to be a minimizer of $J(w)$ we must have 
    \begin{align*}
        J'(w) &= 2(Xw-y)^TX + 2\lambda w^T\\
        &= 2(Xw)^TX - 2y^TX + 2\lambda w^T\\
        &= 2w^TX^TX - 2y^TX + 2\lambda w^T\\
        &= 0\\
    \end{align*}
    
    So then 
    \begin{align*}
        y^TX &= w^TX^TX + \lambda w^T
    \end{align*}
    
    And by taking the transpose of both sides we get 
    \begin{align*}
        X^Ty &= X^TXw + \lambda w
    \end{align*}
    
    \item Solving for $w$ we get
    \begin{align*}
        X^Ty &= (X^TX + \lambda I)w\\
        w &= (X^TX + \lambda I)^{-1}X^Ty
    \end{align*}
    
    \item TODO!!!!!!!!
\end{enumerate}
\end{solution}
\newpage

\begin{problem}{3.2}
    Rewrite $X^{T}Xw+\lambda Iw=X^{T}y$ as $w=\frac{1}{\lambda}(X^{T}y-X^{T}Xw)$.
    Based on this, show that we can write $w=X^{T}\alpha$ for some $\alpha$,
    and give an expression for $\alpha$.
\end{problem}
\begin{solution}{}
    Note that
    \begin{align*}
        w &= \frac{1}{\lambda} (X^Ty - X^TXw)\\
        w &= \frac{1}{\lambda}X^T(y-Xw)\\
    \end{align*}
    Then setting $\alpha = \frac{1}{\lambda}(y-Xw)$ we have
    \begin{align*}
        w=X^{T}\alpha
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.3}
    Based on the fact that $w=X^{T}\alpha$, explain why we say w is ``in
    the span of the data.''
\end{problem}
\begin{solution}{}
    Since $w=X^T\alpha$, we can also express $w$ as $w = \sum_{i=1}^n x_i \alpha_i$. Thus $w$ is a linear combination
    of the training data X and as such is in the span of the training data.
\end{solution}
\newpage

\begin{problem}{3.4}
    Show that $\alpha=(\lambda I+XX^{T})^{-1}y$. Note that $XX^{T}$
    is the kernel matrix for the standard vector dot product.
\end{problem}
\begin{solution}{}
    Observe that
    \begin{align*}
        \alpha &= \frac{1}{\lambda}(y-Xw)\\
        \alpha &= \frac{1}{\lambda}(y-XX^T\alpha)\\
        \lambda \alpha &= y-XX^T\alpha\\
        \lambda \alpha + XX^T\alpha &= y\\
        (\lambda I + XX^T)\alpha &= y\\
        \alpha &= (\lambda I + XX^T)^{-1}y
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.5}
    Give a kernelized expression for the $Xw$, the predicted values on
    the training points.
\end{problem}
\begin{solution}{}
    Recall that
    \begin{align*}
        \alpha &= (\lambda I + XX^T)^{-1}y
    \end{align*}
    Then
    \begin{align*}
        Xw &= XX^T\alpha\\
        &= XX^T(\lambda I + XX^T)^{-1}y \\
        &= K(\lambda I + K)^{-1}y
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.6}
     Give an expression for the prediction $f(x)=x^{T}w^{*}$ for a new
    point $x$, not in the training set. The expression should only involve
    $x$ via inner products with other $x$'s. {[}Hint: It is often convenient
    to define the column vector
    \[
    k_{x}=\begin{pmatrix}x^{T}x_{1}\\
    \vdots\\
    x^{T}x_{n}
    \end{pmatrix}
    \]
    to simplify the expression.{]}
\end{problem}
\begin{solution}{}
    Recall that $w^* = X^T\alpha^*$ and that $\alpha^* = (\lambda I + XX^T)^{-1}y$.
    Also note that
    \begin{align*}
        x^TX^T = \begin{pmatrix}x^{T}x_{1}\\
                    \vdots\\
                    x^{T}x_{n}
                    \end{pmatrix} = k_x
    \end{align*}
    Then
    \begin{align*}
        f(x) &= x^Tw^*\\
        &= x^TX^T\alpha^*\\
        &= x^TX^T(\lambda I + XX^T)y\\
        &= k_x(\lambda I + K)y
    \end{align*}
\end{solution}
\newpage

\section*{4. Pegasos and SSGD for $\ell_{2}$-regularized ERM}
\begin{problem}{4.1}
For each $i=1,\ldots,n$, let $g_{i}(w)$ be a subgradient
of $J_{i}(w)$ at $w\in\reals^{d}$. Let $v_{i}(w)$ be a subgradient
of $\ell_{i}(w)$ at $w$. Give an expression for $g_{i}(w)$ in terms
of $w$ and $v_{i}(w)$
\end{problem}
\begin{solution}{}
    \begin{align*}
        g_i(w) = \lambda w + v_i(w)
    \end{align*}
\end{solution}
\newpage

\begin{problem}{}
Show that $\ex g_{i}(w)\in\partial J(w)$, where the
expectation is over the randomly selected $i\in1,\ldots,n$.
\end{problem}
\begin{solution}{}
    Let $g(w)$ be a subgradient of $J(w)$ and $v_i(w)$ a subgradient of $l_i(w)$ Now observe that
    \begin{align*}
        g(w) &= \lambda w + \frac{1}{n}\sum_{i=1}^n v_i(w)\\
        &= \lambda w + \ex v_i(w)
    \end{align*}
    And that 
    \begin{align*}
        \ex g_i(w) &= \ex \big( \lambda w + v_i(w) \big)\\
        &= \lambda w + \ex v_i(w)
    \end{align*}
    Then $\ex g_i(w) = g(w) \in \partial J(w)$
\end{solution}
\newpage

\begin{problem}{4.3}
Now suppose we are carrying out SSGD with the Pegasos
step-size $\eta^{(t)}=1/\left(\lambda t\right)$, $t=1,2,\ldots$.,
starting from $w^{(1)}=0$. In the $t$'th step, suppose we select
the $i$th point and thus take the step $w^{(t+1)}=w^{(t)}-\eta^{(t)}g_{i}(w^{(t)})$.
Let's write $v^{(t)}=v_{i}(w^{(t)})$, which is the subgradient of
the loss part of $J_{i}(w^{(t)})$ that is used in step $t$. Show
that
\[
w^{(t+1)}=-\frac{1}{\lambda t}\sum_{\tau=1}^{t}v^{(\tau)}
\]
\end{problem}
\begin{solution}{}
$\,$
    \begin{enumerate}
        \item Base case $t=1$\\
            Observe that
            \begin{align*}
                w^{(2)} &= w^{(1)} = \frac{1}{\lambda} * \lambda w^{(1)} - \frac{1}{\lambda} v^{(1)} \\
                &=  - \frac{1}{\lambda} v^{(1)}\\
                &=  - \frac{1}{\lambda \cdot 1} \sum_{\tau=1}^1 v^{(\tau)}\\
                &= - \frac{1}{\lambda t} \sum_{\tau=1}^t v^{(\tau)}
            \end{align*}
        
        \item Induction step.\\
            Assume that $w^{(t)}=-\frac{1}{\lambda (t-1)}\sum_{\tau=1}^{t-1}v^{(\tau)}$. Now observe that
            \begin{align*}
                w^{(t+1)} &= w^{(t)} - \frac{1}{t}w^{(t)} - \frac{1}{\lambda t}v^{(t)}\\
                &= \frac{t-1}{t}w^{(t)} - \frac{1}{\lambda t} v^{(t)}\\
                &= -\frac{1}{\lambda t} \sum_{\tau=1}^{t-1}v^{(\tau)} - \frac{1}{\lambda t}v^{(t)}\\
                &= -\frac{1}{\lambda t} \sum_{\tau=1}^{t}v^{(\tau)}
            \end{align*}
    \end{enumerate}
\end{solution}
\newpage

\begin{problem}{4.a}
Explain how Algorithm 1 can be implemented so that, if
$x_{j}$ has $s$ nonzero entries, then we only need to do $O(s)$
memory accesses in every pass through the loop.
\end{problem}
\begin{solution}{}
    If we implement the algorithm with sparse matrices or dictionaries, then when computing $y_j\langle w^{(t)}, x_j \rangle$
    and $y_jx_j$ we only need to access the $s$ nonzero elements of $x_j$ and their corresponding elements in $y_j$ and $w^{(t)}$.
    The remaining operations are $O(1)$ and so in total we only need to do $O(s)$ memory accesses.
\end{solution}
\newpage
-
\section{5. Kernelized Pegasos}
\begin{problem}{}
Kernelize the expression for the margin. That is, show that $y_{j}\left\langle w^{(t)},x_{j}\right\rangle =y_{j}K_{j\cdot}\alpha^{(t)}$,
where $k(x_{i},x_{j})=\left\langle x_{i},x_{j}\right\rangle $ and
$K_{j\cdot}$ denotes the $j$th row of the kernel matrix $K$ corresponding
to kernel $k$.
\end{problem}
\begin{solution}{}
    Observe that
    \begin{align*}
        \langle w^{(t)}, x_j \rangle &= \langle \sum_{i=1}^n \alpha_i^{(t)}x_i, x_j \rangle\\
        &= \sum_{i=1}^n \alpha_i^{(t)} \langle x_i, x_j \rangle\\
        &= \begin{bmatrix} \langle x_1, x_j \rangle & \dots & \langle x_n,x_j \rangle \end{bmatrix} \alpha^{(t)}\\
        &= \begin{bmatrix} k(x_1,x_j)  & \dots & k(x_n,x_j) \end{bmatrix} \alpha^{(t)}\\
        &= \begin{bmatrix} k(x_j,x_1)  & \dots & k(x_j,x_n) \end{bmatrix} \alpha^{(t)}\\
        &= K_j\alpha^{(t)}
    \end{align*}
    Thus $y_{j}\left\langle w^{(t)},x_{j}\right\rangle =y_{j}K_{j\cdot}\alpha^{(t)}$
\end{solution}
\newpage

\begin{problem}{5.2}
Suppose that $w^{(t)}=\sum_{i=1}^{n}\alpha_{i}^{(t)}x_{i}$ and for
the next step we have selected a point $\left(x_{j},y_{j}\right)$
that does not have a margin violation. Give an update expression for
$\alpha^{(t+1)}$ so that $w^{(t+1)}=\sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}$.
\end{problem}
\begin{solution}{}
    When $\left(x_{j},y_{j}\right)$ does not result in a margin violation, $w^{(t+1)} = (1-\eta^{(t)}\lambda)w^{(t)}$.\\ 
    Then in order to have $w^{(t+1)}=\sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}$, we must also have 
    \begin{align*}
        (1-\eta^{(t)}\lambda)w^{(t)} &= \sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}\\
        (1-\eta^{(t)}\lambda)\sum_{i=1}^{n}\alpha_{i}^{(t)}x_{i} &= \sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}
    \end{align*}
    Now setting $\alpha^{(t+1)} = (1-\eta^{(t)}\lambda)\alpha^{(t)}$ preserves the equality above. Thus this is a valid update rule.
\end{solution}
\newpage

\begin{problem}{5.3}
Repeat the previous problem, but for the case that $\left(x_{j},y_{j}\right)$
has a margin violation. Then give the full pseudocode for kernelized
Pegasos. You may assume that you receive the kernel matrix $K$ as
input, along with the labels $y_{1},\ldots,y_{n}\in\left\{ -1,1\right\}$.
\end{problem}
\begin{solution}{}
    Note that when we have a margin error,
    \begin{align*}
        w^{(t+1)} &= (1-\eta^{(t)}\lambda)w^{(t)} + \eta^{(t)}y_jx_j\\
        &= (1-\eta^{(t)}\lambda)\sum_{i=1}^{n}\alpha_{i}^{(t)}x_{i} + \eta^{(t)}y_jx_j\\
        &= \sum_{i=1}^{n} \big( (1-\eta^{(t)}\lambda)\alpha_{i}^{(t)} +  \mathds{1}[i=j]\eta^{(t)}y_j \big) x_i
    \end{align*}
    Thus an appropriate update rule for $\alpha$ is 
    \begin{align*}
        \alpha_i^{(t+1)} &= (1-\eta^{(t)}\lambda)\alpha_{i}^{(t)} +  \mathds{1}[i=j]\eta^{(t)}y_j
    \end{align*}
\end{solution}
\newpage

% SECTION 7 --------------------------------------------------------------------------------------
\section{7. Representer Theorem}
\begin{problem}{7.1}
Let $M$ be a closed subspace of a Hilbert space $\ch$. For any $x\in\ch$,
let $m_{0}=\proj_{M}x$ be the projection of $x$ onto $M$. By the
Projection Theorem, we know that $(x-m_{0})\perp M$. Then by the
Pythagorean Theorem, we know $\|x\|^{2}=\|m_{0}\|^{2}+\|x-m_{0}\|^{2}$.
From this we concluded in lecture that $\|m_{0}\|\le\|x\|$. Show
that we have $\|m_{0}\|=\|x\|$ only when $m_{0}=x$.
\end{problem}
\begin{solution}{}
    First, assume that $m_0=x$. Then clearly $\|m_{0}\|=\|x\|$.\\
    Now it remains to show that $\|m_{0}\|\neq\|x\|$ when $m_0\neq x$.\\
    \\
    Assume that $m_0\neq x$. Also note that since the inner product is positive-definite,\\ $\|m_0\|^2 \neq \|x\|^2$ iff $\|m_0\| \neq \|x\|$.
    Then
    \begin{align*}
        \|m_0\|^2 &= \|x-(x-m_0)\|^2\\
        &= \|x\|^2 - \|x-m_0\|^2 && \text{Pythagorean Theorem}\\
        &= \|x\|^2 - \langle x-m_0,x-m_0\rangle
    \end{align*}
    Now since $x\neq m_0$, $x-m_0\neq 0$. Then since the inner product is positive-definite,\\ $\langle x-m_0,x-m_0\rangle > 0$.
    Thus $\|m_0\|^2 \neq \|x\|^2$ and so $\|m_0\| \neq \|x\|$.
\end{solution}
\newpage

\begin{problem}{7.2}
Give the proof of the Representer Theorem in the case that $R$ is
strictly increasing. That is, show that if $R$ is strictly increasing,
then all minimizers have this form claimed.
\end{problem}
\begin{solution}{}
    Proof by contradiction:\\
    \\
    Assume $R$ is strictly increasing and $w^*$ is a minimizer of $J(w)$. Let $w_M^* = Proj_Mw$ where $M = span(x_1, \ldots, x_n)$. 
    Note that for any $m\in M, \,\, \exists\,\, \alpha\,\, \text{such that}\,\, m=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$. By way of contradiction 
    also assume that $w^* \notin M$.\\
    
    Since $w^* \notin M$, $w^* \neq w_M^*$. Furthermore, $\|w_M^*\| < \|w^*\|$ since projections reduce norms. Since $R$ is strictly increasing, 
    $R\big(\|w_M^*\|\big) < R\big(\|w^*\|\big)$. Now let $x\in M$ and observe that
    \begin{align*}
        \langle w^*,x\rangle &= \langle w_M^*+w^*-w_M^*, x\rangle\\
        &= \langle w_M^*,x\rangle + \langle w^*-w_M^*, x\rangle\\
        &= \langle w_M^*, x\rangle & \text{since $w^*-w_M^* \perp M$}
    \end{align*}
    Thus 
    \begin{align*}
        L\big(\langle w^*,\psi(x_1)\rangle, \ldots,\langle w^*,\psi(x_n)\rangle\big) = L\big(\langle w_M^*,\psi(x_1)\rangle, \ldots,\langle w_M^*,\psi(x_n)\rangle\big)
    \end{align*}
    Therefore $J(w_M^*) < J(w^*)$ and so $w^*$ is not a minimizer of $J(w)$.\\
    
    We have shown that when $R$ is strictly increasing, we cannot have any minimizers outside of $M$. Thus all minimizers are elements of $M$ 
    and can be written as $w^*=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$.
\end{solution}

\end{document}
