\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{DS-GA 1003}
\newcommand\hwnumber{4}                  % <-- homework number
\newcommand\NetIDa{Cody Fizette}           % <-- NetID of person #1
\newcommand\NetIDb{netid12038}           % <-- NetID of person #2 (Comment this line out for problem sets)

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}
\global\long\def\minimizer#1{#1_{*}}
\global\long\def\ex{\mathbb{E}}

\newenvironment{solution}[2][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}


\section*{2. Kernel Matrices}

\begin{problem}{2.1}
\\
Consider a set of vectors $S=\{x_{1},\ldots,x_{m}\}$.
Let $X$ denote the matrix whose rows are these vectors. Form the
Gram matrix $K=XX^{T}$. Show that knowing $K$ is equivalent to knowing
the set of pairwise distances among the vectors in $S$ as well as
the vector lengths.
\end{problem}



\begin{solution}{}
From the definition of the Gram matrix, knowing $K$ is equivalent to knowing
$\langle x, x' \rangle\,\, \forall\,\, x,x' \in S$. Now let $x,x' \in S$ and observe that
\begin{align*}
    \langle x,x' \rangle &= \langle x'+(x-x'), x+(x'-x) \rangle\\
    &= \langle x', x\rangle + \langle x', x'\rangle + \langle x', -x\rangle + \langle x, x\rangle + \langle -x', x\rangle + \langle x - x', x'-x\rangle\\
    &= \|x\|^2 + \|x'\|^2 - \|x-x', x-x'\|^2 - \langle x',x \rangle\\
    &= \frac{\|x\|^2 + \|x'\|^2 - \|x-x', x-x'\|^2}{2}
\end{align*}

Thus it is possible to express elements of $K$ in terms of the distance between vectors in $S$ as well as the vector lengths. We can conclude that
knowing $K$ is equivalent to knowing the set of pairwise distances among the vectors in $S$ as well as the vector lengths.
\end{solution}
\newpage


\section*{3. Kernel Ridge Regression}
\begin{problem}{3.1}
Show that for $w$ to be a minimizer of $J(w)$, we must have $X^{T}Xw+\lambda Iw=X^{T}y$.
Show that the minimizer of $J(w)$ is $w=(X^{T}X+\lambda I)^{-1}X^{T}y$.
Justify that the matrix $X^{T}X+\lambda I$ is invertible, for $\lambda>0$.
\end{problem}

\begin{solution}{}
$\,$
\newline
\begin{enumerate}
    \item For $w$ to be a minimizer of $J(w)$ we must have 
    \begin{align*}
        J'(w) &= 2(Xw-y)^TX + 2\lambda w^T\\
        &= 2(Xw)^TX - 2y^TX + 2\lambda w^T\\
        &= 2w^TX^TX - 2y^TX + 2\lambda w^T\\
        &= 0\\
    \end{align*}
    
    So then 
    \begin{align*}
        y^TX &= w^TX^TX + \lambda w^T
    \end{align*}
    
    And by taking the transpose of both sides we get 
    \begin{align*}
        X^Ty &= X^TXw + \lambda w
    \end{align*}
    
    \item Solving for $w$ we get
    \begin{align*}
        X^Ty &= (X^TX + \lambda I)w\\
        w &= (X^TX + \lambda I)^{-1}X^Ty
    \end{align*}
    
    \item TODO!!!!!!!!
\end{enumerate}
\end{solution}
\newpage

\begin{problem}{3.2}


\end{problem}





\end{document}
