\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{dsfont}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{DS-GA 1003}
\newcommand\hwnumber{4}                  % <-- homework number
\newcommand\NetIDa{Cody Fizette}           % <-- NetID of person #1
\newcommand\NetIDb{netid12038}           % <-- NetID of person #2 (Comment this line out for problem sets)
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}
\global\long\def\minimizer#1{#1_{*}}
\global\long\def\ex{\mathbb{E}}

\newenvironment{solution}[2][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}


\section*{2. Kernel Matrices}

\begin{problem}{2.1}

Consider a set of vectors $S=\{x_{1},\ldots,x_{m}\}$.
Let $X$ denote the matrix whose rows are these vectors. Form the
Gram matrix $K=XX^{T}$. Show that knowing $K$ is equivalent to knowing
the set of pairwise distances among the vectors in $S$ as well as
the vector lengths.
\end{problem}



\begin{solution}{}
From the definition of the Gram matrix, knowing $K$ is equivalent to knowing
$\langle x, x' \rangle\,\, \forall\,\, x,x' \in S$. Now let $x,x' \in S$ and observe that
\begin{align*}
    \langle x,x' \rangle &= \langle x'+(x-x'), x+(x'-x) \rangle\\
    &= \langle x', x\rangle + \langle x', x'\rangle + \langle x', -x\rangle + \langle x, x\rangle + \langle -x', x\rangle + \langle x - x', x'-x\rangle\\
    &= \|x\|^2 + \|x'\|^2 - \|x-x', x-x'\|^2 - \langle x',x \rangle\\
    &= \frac{\|x\|^2 + \|x'\|^2 - \|x-x', x-x'\|^2}{2}
\end{align*}

Thus it is possible to express elements of $K$ in terms of the distance between vectors in $S$ as well as the vector lengths. We can conclude that
knowing $K$ is equivalent to knowing the set of pairwise distances among the vectors in $S$ as well as the vector lengths.
\end{solution}
\newpage


\section*{3. Kernel Ridge Regression}
\begin{problem}{3.1}
Show that for $w$ to be a minimizer of $J(w)$, we must have $X^{T}Xw+\lambda Iw=X^{T}y$.
Show that the minimizer of $J(w)$ is $w=(X^{T}X+\lambda I)^{-1}X^{T}y$.
Justify that the matrix $X^{T}X+\lambda I$ is invertible, for $\lambda>0$.
\end{problem}

\begin{solution}{}
$\,$
\begin{enumerate}
    \item For $w$ to be a minimizer of $J(w)$ we must have 
    \begin{align*}
        J'(w) &= 2(Xw-y)^TX + 2\lambda w^T\\
        &= 2(Xw)^TX - 2y^TX + 2\lambda w^T\\
        &= 2w^TX^TX - 2y^TX + 2\lambda w^T\\
        &= 0\\
    \end{align*}
    
    So then 
    \begin{align*}
        y^TX &= w^TX^TX + \lambda w^T
    \end{align*}
    
    And by taking the transpose of both sides we get 
    \begin{align*}
        X^Ty &= X^TXw + \lambda w
    \end{align*}
    
    \item Solving for $w$ we get
    \begin{align*}
        X^Ty &= (X^TX + \lambda I)w\\
        w &= (X^TX + \lambda I)^{-1}X^Ty
    \end{align*}
    
    \item TODO!!!!!!!!
\end{enumerate}
\end{solution}
\newpage

\begin{problem}{3.2}
    Rewrite $X^{T}Xw+\lambda Iw=X^{T}y$ as $w=\frac{1}{\lambda}(X^{T}y-X^{T}Xw)$.
    Based on this, show that we can write $w=X^{T}\alpha$ for some $\alpha$,
    and give an expression for $\alpha$.
\end{problem}
\begin{solution}{}
    Note that
    \begin{align*}
        w &= \frac{1}{\lambda} (X^Ty - X^TXw)\\
        w &= \frac{1}{\lambda}X^T(y-Xw)\\
    \end{align*}
    Then setting $\alpha = \frac{1}{\lambda}(y-Xw)$ we have
    \begin{align*}
        w=X^{T}\alpha
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.3}
    Based on the fact that $w=X^{T}\alpha$, explain why we say w is ``in
    the span of the data.''
\end{problem}
\begin{solution}{}
    Since $w=X^T\alpha$, we can also express $w$ as $w = \sum_{i=1}^n x_i \alpha_i$. Thus $w$ is a linear combination
    of the training data X and as such is in the span of the training data.
\end{solution}
\newpage

\begin{problem}{3.4}
    Show that $\alpha=(\lambda I+XX^{T})^{-1}y$. Note that $XX^{T}$
    is the kernel matrix for the standard vector dot product.
\end{problem}
\begin{solution}{}
    Observe that
    \begin{align*}
        \alpha &= \frac{1}{\lambda}(y-Xw)\\
        \alpha &= \frac{1}{\lambda}(y-XX^T\alpha)\\
        \lambda \alpha &= y-XX^T\alpha\\
        \lambda \alpha + XX^T\alpha &= y\\
        (\lambda I + XX^T)\alpha &= y\\
        \alpha &= (\lambda I + XX^T)^{-1}y
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.5}
    Give a kernelized expression for the $Xw$, the predicted values on
    the training points.
\end{problem}
\begin{solution}{}
    Recall that
    \begin{align*}
        \alpha &= (\lambda I + XX^T)^{-1}y
    \end{align*}
    Then
    \begin{align*}
        Xw &= XX^T\alpha\\
        &= XX^T(\lambda I + XX^T)^{-1}y \\
        &= K(\lambda I + K)^{-1}y
    \end{align*}
\end{solution}
\newpage

\begin{problem}{3.6}
     Give an expression for the prediction $f(x)=x^{T}w^{*}$ for a new
    point $x$, not in the training set. The expression should only involve
    $x$ via inner products with other $x$'s. {[}Hint: It is often convenient
    to define the column vector
    \[
    k_{x}=\begin{pmatrix}x^{T}x_{1}\\
    \vdots\\
    x^{T}x_{n}
    \end{pmatrix}
    \]
    to simplify the expression.{]}
\end{problem}
\begin{solution}{}
    Recall that $w^* = X^T\alpha^*$ and that $\alpha^* = (\lambda I + XX^T)^{-1}y$.
    Also note that
    \begin{align*}
        x^TX^T = \begin{pmatrix}x^{T}x_{1}\\
                    \vdots\\
                    x^{T}x_{n}
                    \end{pmatrix} = k_x
    \end{align*}
    Then
    \begin{align*}
        f(x) &= x^Tw^*\\
        &= x^TX^T\alpha^*\\
        &= x^TX^T(\lambda I + XX^T)y\\
        &= k_x(\lambda I + K)y
    \end{align*}
\end{solution}
\newpage

\section*{4. Pegasos and SSGD for $\ell_{2}$-regularized ERM}
\begin{problem}{4.1}
For each $i=1,\ldots,n$, let $g_{i}(w)$ be a subgradient
of $J_{i}(w)$ at $w\in\reals^{d}$. Let $v_{i}(w)$ be a subgradient
of $\ell_{i}(w)$ at $w$. Give an expression for $g_{i}(w)$ in terms
of $w$ and $v_{i}(w)$
\end{problem}
\begin{solution}{}
    \begin{align*}
        g_i(w) = \lambda w + v_i(w)
    \end{align*}
\end{solution}
\newpage

\begin{problem}{}
Show that $\ex g_{i}(w)\in\partial J(w)$, where the
expectation is over the randomly selected $i\in1,\ldots,n$.
\end{problem}
\begin{solution}{}
    Let $g(w)$ be a subgradient of $J(w)$ and $v_i(w)$ a subgradient of $l_i(w)$ Now observe that
    \begin{align*}
        g(w) &= \lambda w + \frac{1}{n}\sum_{i=1}^n v_i(w)\\
        &= \lambda w + \ex v_i(w)
    \end{align*}
    And that 
    \begin{align*}
        \ex g_i(w) &= \ex \big( \lambda w + v_i(w) \big)\\
        &= \lambda w + \ex v_i(w)
    \end{align*}
    Then $\ex g_i(w) = g(w) \in \partial J(w)$
\end{solution}
\newpage

\begin{problem}{4.3}
Now suppose we are carrying out SSGD with the Pegasos
step-size $\eta^{(t)}=1/\left(\lambda t\right)$, $t=1,2,\ldots$.,
starting from $w^{(1)}=0$. In the $t$'th step, suppose we select
the $i$th point and thus take the step $w^{(t+1)}=w^{(t)}-\eta^{(t)}g_{i}(w^{(t)})$.
Let's write $v^{(t)}=v_{i}(w^{(t)})$, which is the subgradient of
the loss part of $J_{i}(w^{(t)})$ that is used in step $t$. Show
that
\[
w^{(t+1)}=-\frac{1}{\lambda t}\sum_{\tau=1}^{t}v^{(\tau)}
\]
\end{problem}
\begin{solution}{}
$\,$
    \begin{enumerate}
        \item Base case $t=1$\\
            Observe that
            \begin{align*}
                w^{(2)} &= w^{(1)} = \frac{1}{\lambda} * \lambda w^{(1)} - \frac{1}{\lambda} v^{(1)} \\
                &=  - \frac{1}{\lambda} v^{(1)}\\
                &=  - \frac{1}{\lambda \cdot 1} \sum_{\tau=1}^1 v^{(\tau)}\\
                &= - \frac{1}{\lambda t} \sum_{\tau=1}^t v^{(\tau)}
            \end{align*}
        
        \item Induction step.\\
            Assume that $w^{(t)}=-\frac{1}{\lambda (t-1)}\sum_{\tau=1}^{t-1}v^{(\tau)}$. Now observe that
            \begin{align*}
                w^{(t+1)} &= w^{(t)} - \frac{1}{t}w^{(t)} - \frac{1}{\lambda t}v^{(t)}\\
                &= \frac{t-1}{t}w^{(t)} - \frac{1}{\lambda t} v^{(t)}\\
                &= -\frac{1}{\lambda t} \sum_{\tau=1}^{t-1}v^{(\tau)} - \frac{1}{\lambda t}v^{(t)}\\
                &= -\frac{1}{\lambda t} \sum_{\tau=1}^{t}v^{(\tau)}
            \end{align*}
    \end{enumerate}
\end{solution}
\newpage

\begin{problem}{4.a}
Explain how Algorithm 1 can be implemented so that, if
$x_{j}$ has $s$ nonzero entries, then we only need to do $O(s)$
memory accesses in every pass through the loop.
\end{problem}
\begin{solution}{}
    If we implement the algorithm with sparse matrices or dictionaries, then when computing $y_j\langle w^{(t)}, x_j \rangle$
    and $y_jx_j$ we only need to access the $s$ nonzero elements of $x_j$ and their corresponding elements in $y_j$ and $w^{(t)}$.
    The remaining operations are $O(1)$ and so in total we only need to do $O(s)$ memory accesses.
\end{solution}
\newpage

% SECTION 5 --------------------------------------------------------------------------------------
\section*{5. Kernelized Pegasos}
\begin{problem}{}
Kernelize the expression for the margin. That is, show that $y_{j}\left\langle w^{(t)},x_{j}\right\rangle =y_{j}K_{j\cdot}\alpha^{(t)}$,
where $k(x_{i},x_{j})=\left\langle x_{i},x_{j}\right\rangle $ and
$K_{j\cdot}$ denotes the $j$th row of the kernel matrix $K$ corresponding
to kernel $k$.
\end{problem}
\begin{solution}{}
    Observe that
    \begin{align*}
        \langle w^{(t)}, x_j \rangle &= \langle \sum_{i=1}^n \alpha_i^{(t)}x_i, x_j \rangle\\
        &= \sum_{i=1}^n \alpha_i^{(t)} \langle x_i, x_j \rangle\\
        &= \begin{bmatrix} \langle x_1, x_j \rangle & \dots & \langle x_n,x_j \rangle \end{bmatrix} \alpha^{(t)}\\
        &= \begin{bmatrix} k(x_1,x_j)  & \dots & k(x_n,x_j) \end{bmatrix} \alpha^{(t)}\\
        &= \begin{bmatrix} k(x_j,x_1)  & \dots & k(x_j,x_n) \end{bmatrix} \alpha^{(t)}\\
        &= K_j\alpha^{(t)}
    \end{align*}
    Thus $y_{j}\left\langle w^{(t)},x_{j}\right\rangle =y_{j}K_{j\cdot}\alpha^{(t)}$
\end{solution}
\newpage

\begin{problem}{5.2}
Suppose that $w^{(t)}=\sum_{i=1}^{n}\alpha_{i}^{(t)}x_{i}$ and for
the next step we have selected a point $\left(x_{j},y_{j}\right)$
that does not have a margin violation. Give an update expression for
$\alpha^{(t+1)}$ so that $w^{(t+1)}=\sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}$.
\end{problem}
\begin{solution}{}
    When $\left(x_{j},y_{j}\right)$ does not result in a margin violation, $w^{(t+1)} = (1-\eta^{(t)}\lambda)w^{(t)}$.\\ 
    Then in order to have $w^{(t+1)}=\sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}$, we must also have 
    \begin{align*}
        (1-\eta^{(t)}\lambda)w^{(t)} &= \sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}\\
        (1-\eta^{(t)}\lambda)\sum_{i=1}^{n}\alpha_{i}^{(t)}x_{i} &= \sum_{i=1}^{n}\alpha_{i}^{(t+1)}x_{i}
    \end{align*}
    Now setting $\alpha^{(t+1)} = (1-\eta^{(t)}\lambda)\alpha^{(t)}$ preserves the equality above. Thus this is a valid update rule.
\end{solution}
\newpage

\begin{problem}{5.3}
Repeat the previous problem, but for the case that $\left(x_{j},y_{j}\right)$
has a margin violation. Then give the full pseudocode for kernelized
Pegasos. You may assume that you receive the kernel matrix $K$ as
input, along with the labels $y_{1},\ldots,y_{n}\in\left\{ -1,1\right\}$.
\end{problem}
\begin{solution}{}
    Note that when we have a margin error,
    \begin{align*}
        w^{(t+1)} &= (1-\eta^{(t)}\lambda)w^{(t)} + \eta^{(t)}y_jx_j\\
        &= (1-\eta^{(t)}\lambda)\sum_{i=1}^{n}\alpha_{i}^{(t)}x_{i} + \eta^{(t)}y_jx_j\\
        &= \sum_{i=1}^{n} \big( (1-\eta^{(t)}\lambda)\alpha_{i}^{(t)} +  \mathds{1}[i=j]\eta^{(t)}y_j \big) x_i
    \end{align*}
    Thus an appropriate update rule for $\alpha$ is 
    \begin{align*}
        \alpha_i^{(t+1)} &= (1-\eta^{(t)}\lambda)\alpha_{i}^{(t)} +  \mathds{1}[i=j]\eta^{(t)}y_j
    \end{align*}
\end{solution}
\newpage

\begin{problem}{5.4}
While the direct implementation of the original Pegasos
required updating all entries of $w$ in every step, a direct kernelization
of Algorithm 2, as we have done above,
leads to updating all entries of $\alpha$ in every step. Give a version
of the kernelized Pegasos algorithm that does not suffer from this
inefficiency. You may try splitting the scale and direction similar
to the approach of the previous problem set, or you may use a decomposition
based on Algorithm 1 from the optional
problem 4 above.
\end{problem}
\begin{solution}{}
    
\end{solution}
\newpage


% SECTION 7 --------------------------------------------------------------------------------------
\section*{7. Representer Theorem}
\begin{problem}{7.1}
Let $M$ be a closed subspace of a Hilbert space $\ch$. For any $x\in\ch$,
let $m_{0}=\proj_{M}x$ be the projection of $x$ onto $M$. By the
Projection Theorem, we know that $(x-m_{0})\perp M$. Then by the
Pythagorean Theorem, we know $\|x\|^{2}=\|m_{0}\|^{2}+\|x-m_{0}\|^{2}$.
From this we concluded in lecture that $\|m_{0}\|\le\|x\|$. Show
that we have $\|m_{0}\|=\|x\|$ only when $m_{0}=x$.
\end{problem}
\begin{solution}{}
    First, assume that $m_0=x$. Then clearly $\|m_{0}\|=\|x\|$.\\
    Now it remains to show that $\|m_{0}\|\neq\|x\|$ when $m_0\neq x$.\\
    \\
    Assume that $m_0\neq x$. Also note that since the inner product is positive-definite,\\ $\|m_0\|^2 \neq \|x\|^2$ iff $\|m_0\| \neq \|x\|$.
    Then
    \begin{align*}
        \|m_0\|^2 &= \|x-(x-m_0)\|^2\\
        &= \|x\|^2 - \|x-m_0\|^2 && \text{Pythagorean Theorem}\\
        &= \|x\|^2 - \langle x-m_0,x-m_0\rangle
    \end{align*}
    Now since $x\neq m_0$, $x-m_0\neq 0$. Then since the inner product is positive-definite,\\ $\langle x-m_0,x-m_0\rangle > 0$.
    Thus $\|m_0\|^2 \neq \|x\|^2$ and so $\|m_0\| \neq \|x\|$.
\end{solution}
\newpage

\begin{problem}{7.2}
Give the proof of the Representer Theorem in the case that $R$ is
strictly increasing. That is, show that if $R$ is strictly increasing,
then all minimizers have this form claimed.
\end{problem}
\begin{solution}{}
    Proof by contradiction:\\
    \\
    Assume $R$ is strictly increasing and $w^*$ is a minimizer of $J(w)$. Let $w_M^* = Proj_Mw$ where $M = span(x_1, \ldots, x_n)$. 
    Note that for any $m\in M, \,\, \exists\,\, \alpha\,\, \text{such that}\,\, m=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$. By way of contradiction 
    also assume that $w^* \notin M$.\\
    
    Since $w^* \notin M$, $w^* \neq w_M^*$. Furthermore, $\|w_M^*\| < \|w^*\|$ since projections reduce norms. Since $R$ is strictly increasing, 
    $R\big(\|w_M^*\|\big) < R\big(\|w^*\|\big)$. Now let $x\in M$ and observe that
    \begin{align*}
        \langle w^*,x\rangle &= \langle w_M^*+w^*-w_M^*, x\rangle\\
        &= \langle w_M^*,x\rangle + \langle w^*-w_M^*, x\rangle\\
        &= \langle w_M^*, x\rangle & \text{since $w^*-w_M^* \perp M$}
    \end{align*}
    Thus 
    \begin{align*}
        L\big(\langle w^*,\psi(x_1)\rangle, \ldots,\langle w^*,\psi(x_n)\rangle\big) = L\big(\langle w_M^*,\psi(x_1)\rangle, \ldots,\langle w_M^*,\psi(x_n)\rangle\big)
    \end{align*}
    Therefore $J(w_M^*) < J(w^*)$ and so $w^*$ is not a minimizer of $J(w)$.\\
    
    We have shown that when $R$ is strictly increasing, we cannot have any minimizers outside of $M$. Thus all minimizers are elements of $M$ 
    and can be written as $w^*=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$.
\end{solution}
\newpage

\begin{problem}{7.3}
Suppose that $R:\reals^{\ge0}\to\reals$ and $L:\reals^{n}\to\reals$
are both convex functions. Use properties of convex functions to \textbf{show
that} $w\mapsto L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right)$
is a convex function of $w$, and then that $J(w)$ is also a convex
function of $w$. For simplicity, you may assume that our feature
space is $\reals^{d}$, rather than a generic Hilbert space. You may
also use the fact that the composition of a convex function and an
affine function is convex. That is, suppose $f:\reals^{n}\to\reals,\ A\in\reals^{n\times m}$
and $b\in\reals^{n}.$ Define $g:\reals^{m}\to\reals$ by $g(x)=f\left(Ax+b\right)$.
Then if $f$ is convex, then so is $g$. From this exercise, \textbf{we
can conclude} that if $L$ and $R$ are convex, then $J$ does have
a minimizer of the form $w^{*}=\sum_{i=1}^{n}\alpha_{i}\psi(x_{i})$,
and if $R$ is also strictly increasing, then all minimizers of $J$
have this form.
\end{problem}
\begin{solution}{}
    Note that in $\reals^{n}$, $\langle x,y\rangle = x^Ty = y^Tx$. Also note that since the sum of convex functions is also convex,
    if $L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right)$ is convex then
    $J(w)$ is convex.\\
    Now observe that 
    \begin{align*}
        \begin{bmatrix}
        \left\langle w,\psi(x_{1})\right\rangle\\
        \vdots\\
        \left\langle w,\psi(x_{n})\right\rangle
        \end{bmatrix}
        &= 
        \begin{bmatrix}
        w^T\psi(x_{1})\\
        \vdots\\
        w^T\psi(x_{n})
        \end{bmatrix}
        =
        \begin{bmatrix}
        \horzbar & \psi(x_{1}) & \horzbar\\
        & \vdots\\
        \horzbar & \psi(x_{n}) & \horzbar
        \end{bmatrix}w
    \end{align*}
    Thus $\begin{bmatrix}
        \left\langle w,\psi(x_{1})\right\rangle\\
        \vdots\\
        \left\langle w,\psi(x_{n})\right\rangle
        \end{bmatrix}$
    can be represented as an affine function of $w$.\\
    
    Since the composition of a convex function and an affine function is also convex,\\ $L\left(\left\langle w,\psi(x_{1})\right\rangle ,\ldots,\left\langle w,\psi(x_{n})\right\rangle \right)$ is a convex function of $w$. Therefore $J(w)$ is convex.
\end{solution}
\newpage

% SECTION 8 --------------------------------------------------------------------------------------
\section*{8. Ivanov and Tikhonov Regularization}
\begin{problem}{8.1}
Suppose that for some $\lambda\ge0$ we have the Tikhonov regularization
solution
\begin{equation}
f^{*}\in\argmin_{f\in\cf}\left[\phi(f)+\lambda\Omega(f)\right].\label{eq:tikhonovReg}
\end{equation}
Show that $f^{*}$ is also an Ivanov solution. That is, $\exists r\ge0$
such that
\begin{equation}
f^{*}\in\argmin_{\substack{f\in\cf}
}\phi(f)\mbox{ subject to }\Omega(f)\le r.\label{eq:ivanovReg}
\end{equation}
\end{problem}
\begin{solution}{}
    Suppose $f^{*}\in\argmin_{f\in\cf}\left[\phi(f)+\lambda\Omega(f)\right]$ and let $r=\Omega(f^*)$. By way of contradiction, 
    suppose that the solution to the Ivanov form is $f'\neq f^*$. Then we have 
    \begin{align*}
        \Omega(f') \leq r = \Omega(f^*)
    \end{align*}
    and
    \begin{align*}
        \phi(f') < \phi(f^*).
    \end{align*}
    Thus
    \begin{align*}
        \phi(f') + \lambda \Omega(f') < \phi(f^*) + \lambda \Omega(f^*).
    \end{align*}
    This contradicts our original assumption that $f^*$ is a minimizer of the Tikhonov problem. Thus $f'=f^*$
    And so $f^*$ is a solution to both the Tikhonov and the Ivanov form of the problem.
\end{solution}
\newpage

\begin{problem}{8.2.1}
Write the Lagrangian $L(w,\lambda)$ for the Ivanov
optimization problem.
\end{problem}
\begin{solution}{}
    \begin{align*}
        L(w,\lambda)=\phi(w)+\lambda(\Omega(w)-r)
    \end{align*}
\end{solution}
$\,$

\begin{problem}{8.2.2}
Write the dual optimization problem in terms of the
dual objective function $g(\lambda)$, and give an expression for
$g(\lambda)$.
\end{problem}
\begin{solution}{}
    The dual optimization problem is to find
    \begin{align*}
        d^*= \sup_{\lambda\succ 0} \inf_w L(w,\lambda) = \sup_{\lambda\succ 0} g(\lambda)
    \end{align*}
    Where $g(\lambda) = \inf_w\big(\phi(w)+\lambda(\Omega(w)-r)\big)$
\end{solution}
\newpage

\begin{problem}{8.2.3}
We assumed that the dual solution is attained, so let
$\lambda^{*}\in\argmax_{\lambda\ge0}g(\lambda)$. We also assumed
strong duality, which implies $\phi(w^{*})=g(\lambda^{*})$. Show
that the minimum in the expression for $g(\lambda^{*})$ is attained
at $w^{*}$.
\textbf{Conclude the proof} by showing that for the choice of $\lambda=\lambda^{*}$,
we have $w^{*}\in\argmin_{w\in\reals^{d}}\left[\phi(w)+\lambda^{*}\Omega(w)\right].$
\end{problem}
\begin{solution}{}
    Observe that
    \begin{align*}
        \phi(w^*) &= g(\lambda^*)\\
        &= \inf_w L(w,\lambda^*)\\
        &\leq  L(w^*,\lambda^*)\\
        &= \phi(w^*) + \lambda^*(\Omega(w^*) - r)\\
        &\leq \phi(w^*) & \text{since $\Omega(w^*)-r\leq0$}
    \end{align*}
    Thus we have equality throughout the expression above and so $\inf_w L(w,\lambda^*) = L(w^*,\lambda^*)$.\\
    We can now conclude that, since $\phi(w^*) = \inf_w \big[\phi(w^*) + \lambda^*(\Omega(w^*) - r)\big]$,
    \begin{align*}
        w^* &\in arg\,min_w\,\, \phi(w) + \lambda^*(\Omega(w)-r)\\
        &= arg\,min_w\,\, \phi(w) + \lambda^*\Omega(w).
    \end{align*}
\end{solution}

\end{document}
